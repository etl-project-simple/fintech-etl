# Fintech ETL Pipeline ğŸš€

An **ETL pipeline** that processes financial transaction data (CSV â†’ Parquet â†’ S3 â†’ Redshift), built with **PySpark**, **Airflow**, **AWS S3**, and **Amazon Redshift**.

---

## ğŸ“‚ Project Structure

```
fintech_project/
â”‚â”€â”€ dags/etl_dags.py         # Airflow DAG defining the daily ETL job
â”‚â”€â”€ etl/
â”‚   â”œâ”€â”€ main.py                # Main entry point to run ETL
â”‚   â”œâ”€â”€ extract.py            # Extract module (CSV â†’ DataFrame)
â”‚   â”œâ”€â”€ transform.py          # Transform module (dim_user, dim_payment, fact_transactions)
â”‚   â””â”€â”€ load.py               # Load module (write to S3 + COPY into Redshift)
â”‚â”€â”€ data/transactions_1M.csv  # Sample input data
â”‚â”€â”€ requirements.txt          # Python dependencies
â”‚â”€â”€ docker-compose.yaml       # Launch Airflow with Docker
â”‚â”€â”€ Dockerfile                # Dockerfile for ETL container
â”‚â”€â”€ .env.simple               # Sample environment file (without secrets)
â”‚â”€â”€ .gitignore
```|

---

## âš™ï¸ Setup

### 1. Clone repository
```bash
git clone https://github.com/duc81/fintech-etl.git
cd fintech-etl
```

### 2. Create `.env` file (based on `.env.simple`)
```bash
cp .env.simple .env
```


### 3. Install dependencies (local)
```bash
pip install -r requirements.txt
```



---

## ğŸš€ Airflow Orchestration

Run Airflow using Docker Compose:
```bash
docker compose up -d
```

- Airflow UI: [http://localhost:8080](http://localhost:8080)  
- DAG: `etl_fintech` â†’ trigger to run ETL pipeline.

---

## ğŸ“Š Outputs

- **S3**: Partitioned Parquet files for each table (`dim_user`, `dim_payment_method`, `fact_transactions`).  
- **Redshift**: Tables loaded under schema `public`.  

---

## ğŸ›¡ï¸ Notes

- `.env.simple` is just a template. Do not commit your real `.env` file.  
- Extendable schema: you can add `dim_time`, `dim_location`, etc. for advanced analysis.  

---

## âœ¨ Author
ğŸ‘¤ **duc81**  
ğŸ“§ `nduc080199@gmail.com`  

â”œ